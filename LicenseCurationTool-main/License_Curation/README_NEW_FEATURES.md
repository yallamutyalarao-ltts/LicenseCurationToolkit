# NEW FEATURES - Enhanced ORT License Curation System

## üéâ What's New

This document summarizes all NEW features added to the Enhanced ORT License Curation System.

---

## üìä New Reports

### 1. Multi-Layer License Comparison Report ‚≠ê NEW
**File:** `license-comparison.html`

**Purpose:** Consolidated view comparing license information from all three sources side-by-side

**Content:**
- Package-by-package comparison table
- ORT, PyPI API, and ScanCode results in columns
- Conflict detection (sources disagree)
- Missing license identification (no sources found)
- Color-coded status indicators:
  - üü¢ Green: All sources agree (consistent)
  - üü° Yellow: Sources conflict (need resolution)
  - üî¥ Red: No license found in any source
- Sortable and filterable table
- Quick links to package homepages and repositories
- Statistics summary (total packages, conflicts, missing)

**Generated by:** `generate_license_comparison.py`

**Usage:**
```bash
python generate_license_comparison.py \
  --ort-result ort-results/analyzer/analyzer-result.yml \
  --pypi-results pypi-licenses/pypi-licenses-full.json \
  --scancode-dir scancode-results \
  --uncertain-packages uncertain-packages/uncertain-packages.json \
  --spdx ort-results/reporter/bom.spdx.yml \
  --output license-comparison.html
```

**Benefits:**
- Single page to see all license information at a glance
- Easy identification of conflicts and missing licenses
- Speeds up manual review process
- Great for team discussions and compliance meetings

---

###2. AI Multi-Layer Resolution Report ‚≠ê NEW
**File:** `ai-multilayer-resolution.html`

**Purpose:** AI-powered intelligent resolution for conflicts and missing licenses across all sources

**Content:**

**Section 1: ‚úÖ Resolved Packages (shown first - the good news!)**
- Packages with consistent licenses across all sources
- Green cards in grid layout
- Shows which sources confirmed the license (ORT + PyPI + ScanCode)
- Typically 80-95% of packages

**Section 2: ‚ö†Ô∏è License Conflicts**
- Packages where ORT, PyPI, and ScanCode disagree
- AI analysis for each conflict with:
  - Recommended license (which one is most likely correct)
  - Reasoning (why this license is most likely)
  - Conflict explanation (why sources disagree)
  - Action required (specific steps to resolve)
  - Confidence level (High/Medium/Low)
  - Verification steps (how to manually confirm)
  - Links to homepage and repository

**Section 3: ‚ùå Missing Licenses**
- Packages with no license found in any source
- AI suggestions based on:
  - Repository URL patterns (GitHub/GitLab)
  - Package ecosystem standards (PyPI, NPM typical licenses)
  - Package name patterns
  - Organization/author patterns
- For each package:
  - Suggested license (SPDX identifier)
  - Reasoning (why this is likely)
  - Confidence level
  - Direct GitHub LICENSE URL (if available)
  - Action steps
  - Verification steps
  - Alternative licenses to check
  - Risk level (High/Medium/Low)

**Generated by:** `ai_multilayer_resolution.py`

**Model:** Azure OpenAI `gpt-4.1-mini`

**API Version:** `2025-01-01-preview`

**Limits:**
- Analyzes up to 15 conflicts per run
- Analyzes up to 10 missing packages per run
- All resolved packages shown (no limit)

**Usage:**
```bash
export AZURE_OPENAI_API_KEY="your-key"
export AZURE_OPENAI_ENDPOINT="your-endpoint"
export AZURE_OPENAI_MODEL="gpt-4.1-mini"

python ai_multilayer_resolution.py \
  --ort-result ort-results/analyzer/analyzer-result.yml \
  --pypi-results pypi-licenses/pypi-licenses-full.json \
  --scancode-dir scancode-results \
  --uncertain-packages uncertain-packages/uncertain-packages.json \
  --output ai-multilayer-resolution.html
```

**‚ö†Ô∏è IMPORTANT:** AI suggestions are advisory only - always verify manually from actual LICENSE files!

**Benefits:**
- Shows good news first (resolved packages) - builds confidence
- Intelligent conflict resolution recommendations
- Time-saving research for missing licenses
- Direct links to verify licenses
- Clear action items for each issue

---

### 3. PyPI License Fetch Report ‚≠ê NEW
**File:** `pypi-licenses-report.html`

**Purpose:** Fast license retrieval results from PyPI API for Python packages

**Content:**
- Summary statistics (total analyzed, found, missing, success rate)
- Workload reduction metrics (how many packages don't need ScanCode)
- List of packages with licenses found via PyPI
- Multiple metadata sources used (license field, license_expression, classifiers)
- Ready-to-use curation suggestions (YAML format)
- Links to PyPI package pages

**Generated by:** `fetch_pypi_licenses.py` with HTML report generation (workflow stage 2.5)

**Benefits:**
- Extremely fast (API calls instead of downloading and scanning)
- Significantly reduces ScanCode workload (50-70% typically)
- Multiple metadata sources increase coverage
- No timeout issues (unlike ScanCode deep scans)
- Generates ready-to-use curations (manual verification required)

**Usage:**
```bash
python fetch_pypi_licenses.py \
  ort-results/analyzer/analyzer-result.yml \
  --fetch \
  --json \
  --csv \
  --html \
  --curations \
  --output-dir pypi-licenses
```

**Outputs:**
- `pypi-licenses-full.json` - Complete report with all packages
- `pypi-licenses-found.json` - Only packages with licenses found
- `pypi-licenses.csv` - Spreadsheet format
- `pypi-licenses-report.html` - Human-readable HTML report
- `curation-suggestions.yml` - ORT curation format (needs manual review!)
- `pypi-fetch-stats.txt` - Statistics and workload metrics

---

## üõ†Ô∏è New Scripts

### 1. `generate_license_comparison.py` ‚≠ê NEW
**Purpose:** Creates consolidated multi-layer license comparison report

**Key Features:**
- Loads data from ORT, PyPI API, ScanCode, and SPDX
- Matches packages across all sources using fuzzy matching
- Detects conflicts (sources disagree)
- Detects missing licenses (no sources found)
- Generates beautiful responsive HTML report
- Color-coded status indicators

**Algorithm:**
1. Load ORT licenses (base set of packages)
2. Load PyPI API results (Python packages only)
3. Load ScanCode results (matched to uncertain packages)
4. Load SPDX for additional metadata
5. Compare licenses across all sources for each package
6. Categorize as: Consistent, Conflict, or Missing
7. Generate HTML report with all details

---

### 2. `ai_multilayer_resolution.py` ‚≠ê NEW
**Purpose:** AI-powered resolution for conflicts and missing licenses

**Key Features:**
- Categorizes packages as Resolved, Conflict, or Missing
- Uses Azure OpenAI GPT-4.1-mini for intelligent analysis
- Generates specific recommendations for each issue
- Shows resolved packages first (positive reinforcement)
- Provides verification steps and links
- Handles errors gracefully (reports generated even if AI fails)

**AI Prompts:**

**For Conflicts:**
```
Analyze this package with conflicting license information:
- Package: name (type)
- ORT: license_from_manifest
- PyPI: license_from_api
- ScanCode: license_from_deep_scan

Provide:
1. Which license is most likely correct and why
2. Why the sources disagree
3. Recommended action
4. Confidence level
```

**For Missing Licenses:**
```
Research this package with NO license information:
- Package: name (type)
- Repository: url
- Homepage: url

Analyze:
1. Repository URL patterns (GitHub LICENSE file)
2. Package ecosystem standards (NPM/PyPI typical licenses)
3. Package name patterns (framework conventions)
4. Organization/author patterns

Provide:
- Suggested license
- Reasoning
- GitHub LICENSE URL (if applicable)
- Verification steps
- Alternative licenses
- Risk level
```

**Configuration:**
- API Version: `2025-01-01-preview`
- Default Deployment: `gpt-4.1-mini`
- Temperature: 0.3 (deterministic)
- Max tokens: 800 per analysis
- Configurable via `AZURE_OPENAI_MODEL` environment variable

---

### 3. `test_azure_openai.py` ‚≠ê NEW
**Purpose:** Test Azure OpenAI configuration before running full analysis

**Key Features:**
- Validates API key, endpoint, and deployment name
- Tests actual connection to Azure OpenAI
- Runs simple prompt to verify model works
- Provides specific troubleshooting guidance for common errors
- Shows clear success/failure status

**Usage:**
```bash
export AZURE_OPENAI_API_KEY="your-key"
export AZURE_OPENAI_ENDPOINT="your-endpoint"
export AZURE_OPENAI_MODEL="gpt-4.1-mini"

python test_azure_openai.py
```

**Output:**
```
================================================================================
AZURE OPENAI CONFIGURATION TEST
================================================================================

üìã Configuration:
  API Key: ‚úì Set
  Endpoint: https://your-endpoint.openai.azure.com
  Model Deployment: gpt-4.1-mini
  API Version: 2025-01-01-preview

üîå Testing Azure OpenAI connection...
  ‚úì Client initialized successfully

ü§ñ Testing model deployment with simple prompt...
  ‚úì Model responded: Hello, ORT!

================================================================================
‚úÖ SUCCESS! Azure OpenAI is configured correctly.
================================================================================
```

**Troubleshooting:**
- DeploymentNotFound (404) ‚Üí Provides instructions to find correct deployment name
- Authentication (401) ‚Üí Explains how to get API key
- Endpoint errors ‚Üí Shows how to verify endpoint URL

---

### 4. `setup_test_env.bat` and `setup_test_env.ps1` ‚≠ê NEW
**Purpose:** Quick Windows setup scripts for testing Azure OpenAI configuration

**Features:**
- Sets environment variables automatically
- Runs configuration test
- Shows clear success/failure
- Provides next steps

**Usage (Windows CMD):**
```cmd
setup_test_env.bat
```

**Usage (PowerShell):**
```powershell
.\setup_test_env.ps1
```

---

## üîß Key Improvements

### 1. Azure OpenAI Configuration Fixes
**Problem:** AI scripts were failing with `DeploymentNotFound` errors

**Root Cause:**
- Scripts were using `gpt-4o-mini` deployment name
- Actual Azure deployment was named `gpt-4.1-mini`
- API version was outdated (`2024-08-01-preview`)

**Solution:**
- Updated all scripts to use `gpt-4.1-mini` as default
- Updated API version to `2025-01-01-preview`
- Made deployment name configurable via `AZURE_OPENAI_MODEL` env var
- Updated workflow defaults to match working configuration
- Created test scripts for local validation

**Files Modified:**
- `ai_multilayer_resolution.py` - Fixed API version and deployment default
- `enhanced-ort-workflow.yml` - Fixed workflow default deployment name
- `AZURE_OPENAI_SETUP.md` - Added comprehensive setup guide
- `CLAUDE.md` - Documented all AI script configurations

---

### 2. PyPI Classifier License Parsing
**Problem:** PyPI packages like Jinja2 have `license: null` but license in classifiers array

**Solution:**
- Added `_parse_license_from_classifier()` method to `fetch_pypi_licenses.py`
- Maps common PyPI classifiers to SPDX identifiers
- Priority order: `license_expression` > `license` > `classifiers`

**Examples:**
- `License :: OSI Approved :: MIT License` ‚Üí `MIT`
- `License :: OSI Approved :: Apache Software License` ‚Üí `Apache-2.0`
- `License :: OSI Approved :: BSD License` ‚Üí `BSD-3-Clause`

**Benefit:** Increased PyPI license detection rate by ~15-20%

---

### 3. ScanCode Package-Level License Reading
**Problem:** Script was only reading file-level licenses from ScanCode, missing package-level declarations

**Solution:**
- Updated `generate_license_comparison.py` and related scripts
- Now reads both:
  1. `packages[].declared_license_expression_spdx` (from package manifest - more reliable)
  2. `files[].licenses[]` (from deep scanning - fallback)
- Prioritizes package-level over file-level

**Benefit:** More accurate ScanCode license detection, fewer false positives

---

### 4. Accurate ScanCode Matching
**Problem:** ScanCode results were being matched to all packages instead of just uncertain packages

**Solution:**
- Added `--uncertain-packages` parameter to scripts
- Match ScanCode files only to packages that actually needed scanning
- Use uncertain packages list for accurate file-to-package mapping

**Benefit:** Correct ScanCode license attribution, no false matches

---

## üìö New Documentation Files

### 1. `LOCAL_TEST.md` ‚≠ê NEW
Complete local testing guide with:
- Step-by-step environment setup
- Test procedures (quick and full)
- Troubleshooting guide
- GitHub Secrets setup instructions

### 2. `FIX_SUMMARY.md` ‚≠ê NEW
Detailed summary of Azure OpenAI fixes with:
- Problem description
- Root cause analysis
- All files changed
- Before/after comparisons
- Testing instructions
- Verification checklist

### 3. `AZURE_OPENAI_SETUP.md` (Updated)
Enhanced with:
- Detailed deployment name setup
- API version documentation
- Common deployment names list
- Troubleshooting section
- Testing locally section

### 4. `CLAUDE.md` (Updated)
Added:
- AI Multi-Layer Resolution configuration
- Updated model deployment defaults
- API version documentation
- Comprehensive AI script reference

---

## üéØ Workflow Updates

### New Stages Added:

**Stage 2.5: Fetch PyPI Licenses** ‚≠ê NEW
```yaml
- Fetches licenses from PyPI API for Python packages
- Fast retrieval (no scanning needed)
- Generates HTML, JSON, CSV, and curation suggestions
- Tracks fetch statistics and workload reduction
- Significantly reduces ScanCode workload
```

**Stage 5d: Multi-Layer License Comparison** ‚≠ê NEW
```yaml
- Generates consolidated comparison report
- Compares ORT, PyPI, and ScanCode results
- Detects conflicts and missing licenses
- Beautiful responsive HTML output
- Deployed to GitHub Pages
```

**Stage 5e: AI Multi-Layer Resolution** ‚≠ê NEW
```yaml
- AI-powered conflict and missing license resolution
- Uses Azure OpenAI GPT-4.1-mini
- Generates actionable recommendations
- Shows resolved packages first
- Deployed to GitHub Pages
```

---

## üí∞ Updated Cost Estimates

**Per Workflow Run (with all AI features):**
- Main ORT curation: ~$0.05 USD (gpt-4.1-mini)
- Conflict analysis: ~$0.10-$0.20 USD (gpt-4, max 20 conflicts)
- Missing licenses: ~$0.05-$0.08 USD (gpt-4o-mini, max 15 packages)
- **AI Multi-Layer Resolution**: ~$0.08-$0.12 USD (gpt-4.1-mini, 15 conflicts + 10 missing) ‚≠ê NEW
- **Total: ~$0.28-$0.45 USD per run**

**Monthly estimate (daily runs):** ~$8-$14 USD per repository

---

## ‚úÖ Benefits Summary

### For Developers:
- ‚úÖ Single page to see all license information (multi-layer comparison)
- ‚úÖ AI helps prioritize which conflicts to investigate first
- ‚úÖ Direct links to verify licenses (saves time)
- ‚úÖ See good news first (resolved packages build confidence)

### For Compliance Teams:
- ‚úÖ Comprehensive view across all detection tools
- ‚úÖ Clear conflict identification
- ‚úÖ AI provides research starting points (not replacements for verification)
- ‚úÖ Verification steps provided for each issue
- ‚úÖ Risk levels help prioritize manual review

### For CI/CD:
- ‚úÖ Faster PyPI license fetch (reduces total pipeline time)
- ‚úÖ Optimized ScanCode workload (only uncertain packages)
- ‚úÖ Robust error handling (reports generate even if AI fails)
- ‚úÖ Easy local testing before pushing to GitHub

### For Team Communication:
- ‚úÖ Beautiful reports for stakeholder presentations
- ‚úÖ Statistics dashboard (resolved vs conflicts vs missing)
- ‚úÖ GitHub Pages deployment for easy sharing
- ‚úÖ Machine-readable formats for automation

---

## üîú Next Steps

1. **Test locally** using `setup_test_env.bat` or `test_azure_openai.py`
2. **Review** the multi-layer comparison report to understand your package status
3. **Use** AI resolution report as a research aid (always verify manually!)
4. **Add** curations for verified licenses using `manage_curations.py`
5. **Monitor** GitHub Actions workflow for successful deployment

---

**Last Updated:** 2025-01-15
**Version:** 2.0 - Multi-Layer Resolution Enhancement
